[
  {
    "filename": "2504.14332v1.pdf",
    "title": "DUSTY STELLAR SOURCES CLASSIFICATION BY IMPLEMENTING MACHINE LEARNING METHODS BASED ON SPECTROSCOPIC OBSERVATIONS IN THE MAGELLANIC CLOUDS",
    "abstract": "Dusty stellar point sources are a significant stage in stellar evolution and contribute to the metal enrichment of galaxies. These objects can be classified using photometric and spectroscopic observations using color-magnitude diagrams (CMD) and infrared excesses in spectral energy distributions (SED). We employed supervised machine learning spectral classification to categorize dusty stellar point sources, including young stellar objects (YSOs) and evolved stars comprising oxygen-and carbon-rich asymptotic giant branch stars (AGBs), red supergiants (RSGs), and post-AGB (PAGB) in the Large and Small Magellanic Clouds, based on spectroscopic labeled data derived from the Surveying the Agents of Galaxy Evolution (SAGE) project, which involved 12 multiwavelength filters and 618 stellar objects. Despite dealing with missing values and uncertainties in the SAGE spectral datasets, we achieved accurate classifications of these sources. To address the challenge of working with small and imbalanced spectral catalogs, we utilized the Synthetic Minority Oversampling Technique (SMOTE), which generates synthetic data points. Subsequently, among all the models applied before and after data augmentation, the Probabilistic Random Forest (PRF) classifier, a tuned Random Forest (RF), demonstrated the highest total accuracy, reaching 89% based on the recall metric in categorizing dusty stellar sources. In this study, using the SMOTE technique does not improve the accuracy of the best model for the CAGB, PAGB, and RSG classes; it stays at 100%, 100%, and 88%, respectively. However, there are variations in the OAGB and YSO classes. Accordingly, we collected photometrically labeled data with properties similar to the training dataset and classified them using the top four PRF models with an accuracy of more than 87%. We also collected multiwavelength data from several studies to classify them using our consensus model, which integrates four top models to present common labels as the final prediction.",
    "main_topic": 1,
    "topic_score": 0.9950000047683716
  },
  {
    "filename": "2504.14337v1.pdf",
    "title": "Multispectral airborne laser scanning for tree species classification: a benchmark of machine learning and deep learning algorithms Preprint",
    "abstract": "Climate-smart and biodiversity-preserving forestry demands precise information on forest resources, extending to the individual tree level. Multispectral airborne laser scanning (ALS) has shown promise in automated point cloud processing and tree segmentation, but challenges remain in identifying rare tree species and leveraging deep learning techniques. This study addresses these gaps by conducting a comprehensive benchmark of machine learning and deep learning methods for tree species classification. For the study, we collected high-density multispectral ALS data (> 1000 pts/m 2 ) at three wavelengths using the FGI-developed HeliALS system, complemented by existing Optech Titan data (35 pts/m 2 ), to evaluate the species classification accuracy of various algorithms in a test site located in Southern Finland. We established a field reference dataset of over 6000 trees across nine species using a newly developed browser-based crowdsourcing tool, which facilitated efficient data annotation. The ALS data, including a training dataset of 1065 segments, was shared with the scientific community to foster collaborative research and diverse algorithmic contributions. Based on 5261 test segments, our findings demonstrate that point-based deep learning methods, particularly a point transformer model, outperformed traditional machine learning and image-based deep learning approaches on high-density multispectral point clouds. For the high-density ALS dataset, a point transformer model provided the best performance reaching an overall (macro-average) accuracy of 87.9% (74.5%) with a training set of 1065 segments and 92.0% (85.1%) with 5000 training segments. The best image-based deep learning method, DetailView, reached an overall (macro-average) accuracy of 84.3% (63.9%), whereas a random forest (RF) classifier achieved an overall (macro-average) accuracy of 83.2% (61.3%). For the sparser ALS dataset, an RF model topped the list with an overall (macro-average) accuracy of 79.9% (57.6%), closely followed by the point transformer at 79.6% (56.0%). Importantly, the overall classification accuracy of the point transformer model on the HeliALS data increased from 73.0% with no spectral information to 84.7% with single-channel reflectance, and to 87.9% with spectral information of all the three channels. By studying the classification accuracy as a function of point density, we conclude that the multispectral information seems especially important for sparse point clouds with 1-50 pts/m 2 . Furthermore, we observed that the classification error follows a power law ε(m) ≈ m -α as a function of the training set size m for both the point transformer model and a RF classifier, with the scaling exponent α being approximately twice higher for the point transformer. By extrapolating the fitted scaling laws, we estimate that a macro-average accuracy of 90% would require 14 000 training trees with the point transformer and 4.9 × 10 6 training trees with the RF classifier, thus highlighting the advantage of deep learning approaches on large datasets.",
    "main_topic": 8,
    "topic_score": 0.9959999918937683
  },
  {
    "filename": "2504.14378v1.pdf",
    "title": "Machine learning enhanced atom probe tomography analysis: a snapshot review",
    "abstract": "Atom probe tomography (APT) is a burgeoning characterization technique that provides compositional mapping of materials in three-dimensions at near-atomic scale. Since its significant expansion in the past 30 years, we estimate that one million APT datasets have been collected, each containing millions to billions of individual ions. Their analysis and the extraction of microstructural information has largely relied upon individual users whose varied level of expertise causes clear and documented bias. Current practices hinder efficient data processing, and make challenging standardization and the deployment of data analysis workflows that would be compliant with FAIR data principles. Over the past decade, building upon the long-standing expertise of the APT community in the development of advanced data processing or \"data mining\" techniques, there has been a surge of novel machine learning (ML) approaches aiming for userindependence, and that are efficient, reproducible, and robust from a statistics perspective. Here, we provide a snapshot review of this rapidly evolving field. We begin with a brief introduction to APT and the nature of the APT data. This is followed by an overview of relevant ML algorithms and a comprehensive review of their applications to APT. We also discuss how ML can enable discoveries beyond human capability, offering new insights into the mechanisms within materials. Finally, we provide guidance for future directions in this domain.",
    "main_topic": 4,
    "topic_score": 0.7739999890327454
  },
  {
    "filename": "2504.14426v1.pdf",
    "title": "Predicting Halo Formation Time Using Machine Learning",
    "abstract": "Context. The formation time of dark-matter halos quantifies their mass assembly history, and as such, it directly impacts the structural and dynamical properties of galaxies within and even influences their galaxy evolution. Despite its importance, halo formation time is not directly observable, necessitating the use of indirect observational proxies-often based on star formation history or galaxy spatial distributions. Recent advancements in machine learning allow for a more comprehensive analysis of galaxy and halo properties, making it possible to develop models for more accurate prediction of halo formation times. Aims. This study aims to investigate a machine learning-based approach to predict halo formation time-defined as the epoch when a halo accretes half of its current mass-using both halo and baryonic properties derived from cosmological simulations. By incorporating properties associated with the brightest cluster galaxy located at the cluster center, its associated intracluster light component and satellite galaxies, we aim to surpass these analytical predictions, improve prediction accuracy and identify key properties that can provide the best proxy for the halo assembly history. Methods. Using The Three Hundred cosmological hydrodynamical simulations, we train Random Forest (RF) and Convolutional Neural Network (CNN) models. The random forest models are trained using a range of dark matter halo and baryonic properties, including halo mass, concentration, stellar and gas masses, and properties of the brightest cluster galaxy and intracluster light within different radial apertures, while CNNs are trained on two-dimensional radial property maps generated by binning particles as a function of radius. Based on these results, we also construct simple linear models that incrementally incorporate observationally accessible features, optimizing for minimal bias and scatter for predicting the halo formation time. Results. Our RF models demonstrated median biases between 4% and 9% with relative error standard deviations around 20% in the prediction of the halo formation time. CNN models trained on two-dimensional property maps, further reduced the median bias to ≲ 4%, though with a higher scatter than the random forest models. With our simple linear models, one can easily predict the halo formation time with only a limited few observables with the bias and scatter compatible with RF results. Lastly, we also show that the traditional relations between halo formation time and halo mass or concentration are well preserved with our predicted values.",
    "main_topic": 6,
    "topic_score": 0.9959999918937683
  },
  {
    "filename": "2504.14473v1.pdf",
    "title": "Invariance-embedded Machine Learning Sub-grid-scale Stress Models for Meso-scale Hurricane Boundary Layer Flow Simulation I: Model Development and a priori Studies",
    "abstract": "This study develops invariance-embedded machine learning sub-grid-scale (SGS) stress models admitting turbulence kinetic energy (TKE) backscatter towards more accurate large eddy simulation (LES) of meso-scale turbulent hurricane boundary layer flows. The new machine learning SGS model consists of two parts: a classification model used to distinguish regions with either strong energy cascade or energy backscatter from those with mild TKE transfer and a regression model used to calculate SGS stresses in regions with strong TKE transfer. To ease model implementation in computational fluid dynamics (CFD) solvers, the Smagorinsky model with a signed coefficient C s , where a positive value indicates energy cascade while a negative one indicates energy backscatter, is employed as the carrier of the machine learning model. To improve its robustness and generality, both physical invariance and geometric invariance features of turbulent flows are embedded into the model input for classification and regression, and the signed Smagorinsky model coefficient is used as the output of the regression model. Different machine-learning methods and input setups have been used to test the classification model's performance. The F1-scores, which measure balanced precision and recall of a model, of the classification models with physical and geometric invariance embedded can be improved by about 17% over those without considering geometric invariance. Regression models based on ensemble neural networks have demonstrated superior performance in predicting the signed Smagorinsky model coefficient, exceeding that of the dynamic Smagorinsky model in a priori tests.",
    "main_topic": 8,
    "topic_score": 0.9940000176429749
  },
  {
    "filename": "2504.14961v1.pdf",
    "title": "Mitigating error cancellation in density functional approximations via machine learning correction",
    "abstract": "The integration of machine learning (ML) with density functional theory has emerged as a promising strategy to enhance the accuracy of density functional methods. While practical implementations of density functional approximations (DFAs) often exploit error cancellation between chemical species to achieve high accuracy in thermochemical and kinetic energy predictions, this approach is inherently system-dependent, which severely limits the transferability of DFAs. To address this challenge, we develop a novel ML-based correction to the widely used B3LYP functional, directly targeting its deviations from the exact exchange-correlation functional. By utilizing highly accurate absolute energies as exclusive reference data, our approach eliminates the reliance on error cancellation. To optimize the ML model, we attribute errors to real-space pointwise contributions and design a double-cycle protocol that incorporates self-consistent-field calculations into the training workflow. Numerical tests demonstrate that the ML model, trained solely on absolute energies, improves the accuracy of calculated relative energies, demonstrating that robust DFAs can be constructed without resorting to error cancellation. Comprehensive benchmarks further show that our ML-corrected B3LYP functional significantly outperforms the original B3LYP across diverse thermochemical and kinetic energy calculations, offering a versatile and superior alternative for practical applications.",
    "main_topic": 3,
    "topic_score": 0.9929999709129333
  },
  {
    "filename": "2504.15034v1.pdf",
    "title": "Predicting Methane Adsorption in Metal-Substituted MOFs: A Comparative Study between Density Functional Theory and Machine Learning",
    "abstract": "Metal-organic frameworks (MOFs) are promising materials for methane capture due to their high surface area and tunable properties. Metal substitution represents a powerful strategy to enhance MOF performance, yet systematic exploration of the vast chemical space remains challenging. In this work, we compare density functional theory (DFT) and machine learning (ML) in predicting methane adsorption properties in metal-substituted variants of three high-performing MOFs: M-HKUST-1, M-ATC, and M-ZIF-8 (M = Cu, Zn). DFT calculations reveal significant differences in methane binding energetics between Cu and Zn variants of all three MOFs. On the other hand, we fine-tuned a pretrained multi-modal ML model, PMTransformer, on a curated subset of hypothetical MOF (hMOF) structures to predict macroscopic adsorption properties. While the model qualitatively predicts adsorption properties for original unaltered MOFs, it fails to distinguish between metal variants despite their different binding energetics identified by DFT. We trace this limitation to the hMOF training data generated using Grand Canonical Monte Carlo (GCMC) simulations based on classical force fields (UFF/TraPPE). Our study highlights a key challenge in ML-based MOF screening: ML models inherit the limitations of their training data, particularly when electronic effects significantly impact adsorption behavior. Our findings emphasize the need for improved force fields or hybrid GCMC/DFT datasets to incorporate both geometric and electronic factors for accurate prediction of adsorption properties in metal-substituted MOFs.",
    "main_topic": 4,
    "topic_score": 0.9940000176429749
  },
  {
    "filename": "2504.15185v1.pdf",
    "title": "ForgeBench: A Machine Learning Benchmark Suite and Auto-Generation Framework for Next-Generation HLS Tools",
    "abstract": "Although High-Level Synthesis (HLS) has attracted considerable interest in hardware design, it has not yet become mainstream due to two primary challenges. First, current HLS hardware design benchmarks are outdated as they do not cover modern machine learning (ML) applications, preventing the rigorous development of HLS tools on ML-focused hardware design. Second, existing HLS tools are outdated because they predominantly target individual accelerator designs and lack an architecture-oriented perspective to support common hardware module extraction and reuse, limiting their adaptability and broader applicability. Motivated by these two limitations, we propose ForgeBench, an ML-focused benchmark suite with a hardware design auto-generation framework for next-generation HLS tools. In addition to the auto-generation framework, we provide two ready-to-use benchmark suites. The first contains over 6,000 representative ML HLS designs. We envision future HLS tools being architecture-oriented, capable of automatically identifying common computational modules across designs, and supporting flexible dataflow and control. Accordingly, the second benchmark suite includes ML HLS designs with possible resource sharing manually implemented to highlight the necessity of architecture-oriented design, ensuring it is future-HLS ready. ForgeBench is open-sourced at  https://github.com/hchen799/ForgeBench .",
    "main_topic": 4,
    "topic_score": 0.9919999837875366
  },
  {
    "filename": "2504.15240v1.pdf",
    "title": "Conformalized-KANs: Uncertainty Quantification with Coverage Guarantees for Kolmogorov-Arnold Networks (KANs) in Scientific Machine Learning",
    "abstract": "This paper explores uncertainty quantification (UQ) methods in the context of Kolmogorov-Arnold Networks (KANs). We apply an ensemble approach to KANs to obtain a heuristic measure of UQ, enhancing interpretability and robustness in modeling complex functions. Building on this, we introduce Conformalized-KANs, which integrate conformal prediction, a distribution-free UQ technique, with KAN ensembles to generate calibrated prediction intervals with guaranteed coverage. Extensive numerical experiments are conducted to evaluate the effectiveness of these methods, focusing particularly on the robustness and accuracy of the prediction intervals under various hyperparameter settings. We show that the conformal KAN predictions can be applied to recent extensions of KANs, including Finite Basis KANs (FBKANs) and multifideilty KANs (MFKANs). The results demonstrate the potential of our approaches to significantly improve the reliability and applicability of KANs in scientific machine learning. * Equal contribution. Preprint. Under review.",
    "main_topic": 4,
    "topic_score": 0.9900000095367432
  },
  {
    "filename": "2504.15287v1.pdf",
    "title": "Continuum Damage Modeling of Biaxial Fatigue Failure in Whole Bone: A Hybrid Approach with Machine Learning Integration",
    "abstract": "Repetitive loading of bone is associated with microdamage accumulation and material property degradation that may ultimately result in fatigue fracture. Our previous work used continuum damage mechanics (CDM)-based finite element (FE) modeling to predict stiffness loss and fatigue failure in whole bone; however, this model did not account for inter-specimen variability in fatigue behaviour and multiaxial loading effects, which limited its applied efficacy. In this study, we refined the CDM-based FE model to predict experimental fatigue-life measurements from 21 whole rabbit tibiae subjected to cyclic axial compression with different magnitudes of superposed torsion. Machine learning (ML) methods were used to predict damage parameters from initial, undamaged, conditions, which were then used to predicted stiffness degradation and fatigue failure with the CDM-based FE modeling pipeline. Regression analysis was used to compare CDM-based FE fatigue-life predictions with experimental measurements. A random forest ML model predicted specimen-specific damage parameters with high accuracy (R² = 0.85) and the CDM-based FE models demonstrated remarkable predictive capability, explaining up to 91% of the variance in fatigue-life measurements. Stiffness degradation profiles also followed a similar trend to experimental measurements; however, this agreement worsened with the level of superposed torsion suggesting additional refinements to the model may be necessary. These findings demonstrate the efficacy of integrating ML with CDM-based FE modeling to predict fatigue life and stiffness degradation. The observed agreement with experimental measurements suggests the modeling framework may provide valuable information regarding the mechanisms of fatigue fracture in whole bone.",
    "main_topic": 3,
    "topic_score": 0.9940000176429749
  },
  {
    "filename": "2504.15304v1.pdf",
    "title": "Can Machine Learning Agents Deal with Hard Choices?",
    "abstract": "Machine Learning (ML) agents have been increasingly used in decision-making across a wide range of tasks and environments. These ML agents are typically designed to balance multiple objectives when making choices. Understanding how their decision-making processes align with or diverge from human reasoning is essential. Human agents often encounter hard choices-situations where options are incommensurable; neither option is preferred, yet the agent is not indifferent between them. In such cases, human agents can identify hard choices and resolve them through deliberation. In contrast, current ML agents, due to fundamental limitations in Multi-Objective Optimisation (MOO) methods, cannot identify hard choices, let alone resolve them. Neither Scalarised Optimisation nor Pareto Optimisation-the two principal MOO approaches-, can capture incommensurability. This limitation generates three distinct alignment problems: the alienness of ML decision-making behaviour from a human perspective; the unreliability of preference-based alignment strategies for hard choices; and the blockage of alignment strategies pursuing multiple objectives. Evaluating two potential technical solutions, I recommend an ensemble solution that appears most promising for enabling ML agents to identify hard choices and mitigate alignment problems. However, no known technique allows ML agents to resolve hard choices through deliberation, as they cannot autonomously change their goals. This underscores the distinctiveness of human agency and urges ML researchers to reconceptualise machine autonomy and develop frameworks and methods that can better address this fundamental gap.",
    "main_topic": 4,
    "topic_score": 0.9940000176429749
  },
  {
    "filename": "2504.15310v1.pdf",
    "title": "Engineering Applications of Artificial Intelligence",
    "abstract": "Power transformers play a critical role within the electrical power system, making their health assessment and the prediction of their remaining lifespan paramount for the purpose of ensuring efficient operation and facilitating effective maintenance planning. This paper undertakes a comprehensive examination of existent literature, with a primary focus on both conventional and cutting-edge techniques employed within this domain. The merits and demerits of recent methodologies and techniques are subjected to meticulous scrutiny and explication. Furthermore, this paper expounds upon intelligent fault diagnosis methodologies and delves into the most widely utilized intelligent algorithms for the assessment of transformer conditions. Diverse Artificial Intelligence (AI) approaches, including Artificial Neural Networks (ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM), Random Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization (PSO), are elucidated offering pragmatic solutions for enhancing the performance of transformer fault diagnosis. The amalgamation of multiple AI methodologies and the exploration of timeseries analysis further contribute to the augmentation of diagnostic precision and the early detection of faults in transformers. By furnishing a comprehensive panorama of AI applications in the field of transformer fault diagnosis, this study lays the groundwork for future research endeavors and the progression of this critical area of study.",
    "main_topic": 9,
    "topic_score": 0.9929999709129333
  },
  {
    "filename": "2504.15465v1.pdf",
    "title": "LithOS: An Operating System for Efficient Machine Learning on GPUs",
    "abstract": "The surging demand for GPUs in datacenters for machine learning (ML) workloads has made efficient GPU utilization crucial. However, meeting the diverse needs of individual ML models while optimizing resource usage is challenging. To enable transparent, fine-grained management of GPU resources that maximizes GPU utilization and energy efficiency while maintaining strong isolation, an operating systems (OS) approach is needed. Hence this paper introduces LithOS, a first step towards a GPU OS. LithOS includes the following new abstractions and mechanisms for efficient GPU resource management: (i) a novel TPC Scheduler that supports spatial scheduling at the granularity of individual TPCs, unlocking efficient TPC stealing between workloads; (ii) transparent kernel atomization to reduce head-of-line blocking and allow dynamic resource reallocation mid-execution; (iii) a lightweight hardware rightsizing mechanism that dynamically determines the minimal TPC resources needed per atom; and (iv) a transparent power management mechanism that reduces power consumption based upon in-flight work characteristics. We implement LithOS in Rust and evaluate its performance across a broad set of deep learning environments, comparing it to state-of-the-art solutions from NVIDIA and prior research. For inference stacking, LithOS reduces tail latencies by 13× compared to MPS; compared to the bestperforming SotA, it reduces tail latencies by 3× while improving aggregate throughput by 1.6×. Furthermore, in hybrid inference-training stacking, LithOS reduces tail latencies by 4.7× compared to MPS; compared to the best-performing SotA, it reduces tail latencies by 1.18× while improving aggregate throughput by 1.35×. Finally, for a modest performance hit under 4%, LithOS's hardware right-sizing provides a quarter of GPU capacity savings on average, while for a 7% hit, LithOS's transparent power management delivers a quarter of a GPU total energy savings on average. Overall, LithOS transparently increases GPU efficiency, establishing a foundation for future OS research on GPUs.",
    "main_topic": 0,
    "topic_score": 0.9950000047683716
  },
  {
    "filename": "2504.15507v2.pdf",
    "title": "Automatically Detecting Numerical Instability in Machine Learning Applications via Soft Assertions",
    "abstract": "Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large/small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety/error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool Soft Assertion Fuzzer found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted \"tumor\", but instead, it incorrectly predicted \"no tumor\" due to the numerical bugs. Our replication package is located at  https://figshare.com/s/6528d21ccd28bea94c32 . CCS Concepts: • Software and its engineering → Software testing and debugging; Software verification and validation.",
    "main_topic": 6,
    "topic_score": 0.9950000047683716
  },
  {
    "filename": "2504.15670v1.pdf",
    "title": "Applicability evaluation of selected xAI methods for machine learning algorithms for signal parameters extraction",
    "abstract": "Machine learning methods find growing application in the reconstruction and analysis of data in high energy physics experiments. A modified convolutional autoencoder model was employed to identify and reconstruct the pulses from scintillating crystals. The model was further investigated using four xAI methods for deeper understanding of the underlying reconstruction mechanism. The results are discussed in detail, underlining the importance of xAI for knowledge gain and further improvement of the algorithms.",
    "main_topic": 5,
    "topic_score": 0.9800000190734863
  },
  {
    "filename": "2504.15710v1.pdf",
    "title": "Prediction of CO2 reduction reaction intermediates and products on transition metal-doped γ-GeSe monolayers: A combined DFT and machine learning approach",
    "abstract": "The electrocatalytic CO2 reduction reaction (CO2RR) is a complex multi-proton-electron transfer process that generates a vast network of reaction intermediates. Accurate prediction of free energy changes (∆G) of these intermediates and products is essential for evaluating catalytic performance. We combined density functional theory (DFT) and machine learning (ML) to screen 25 single-atom catalysts (SACs) on defective γ-GeSe monolayers for CO2 reduction to methanol, methane, and formic acid. Among nine ML models evaluated with 14 intrinsic and DFT-based features, the XGBoost performed best (R 2 = 0.92 and MAE = 0.24 eV), aligning closely with DFT calculations and identifying Ni, Ru, and Rh@GeSe as prospective catalysts. Feature importance analysis in free energy and product predictions highlighted the significance of CO2 activation with O-C-O and IP C-O1 as the key attributes. Furthermore, by incorporating non-DFT-based features, rapid predictions became possible, and the XGBoost model retained its predictive performance with R 2 = 0.89 and MAE = 0.29 eV. This accuracy was further validated using Ir@GeSe. Our work highlights effective SACs for CO2RR, and provides valuable insights for efficient catalyst design.",
    "main_topic": 8,
    "topic_score": 0.9679999947547913
  },
  {
    "filename": "2504.15834v1.pdf",
    "title": "CAUSAL MACHINE LEARNING FOR HIGH-DIMENSIONAL MEDIATION ANALYSIS USING INTERVENTIONAL EFFECTS MAPPED TO A TARGET TRIAL A PREPRINT",
    "abstract": "Causal mediation analysis examines causal pathways linking exposures to disease. The estimation of interventional effects, which are mediation estimands that overcome certain identifiability problems of natural effects, has been advanced through causal machine learning methods, particularly for high-dimensional mediators. Recently, it has been proposed interventional effects can be defined in each study by mapping to a target trial assessing specific hypothetical mediator interventions. This provides an appealing framework to directly address real-world research questions about the extent to which such interventions might mitigate an increased disease risk in the exposed. However, existing estimators for interventional effects mapped to a target trial rely on singly-robust parametric approaches, limiting their applicability in high-dimensional settings. Building upon recent developments in causal machine learning for interventional effects, we address this gap by developing causal machine learning estimators for three interventional effect estimands, defined by target trials assessing hypothetical interventions inducing distinct shifts in joint mediator distributions. These estimands are motivated by a case study within the Longitudinal Study of Australian Children, used for illustration, which assessed how intervening on high inflammatory burden and other noninflammatory adverse metabolomic markers might mitigate the adverse causal effect of overweight or obesity on high blood pressure in adolescence. We develop one-step and (partial) targeted minimum loss-based estimators based on efficient influence functions of those estimands, demonstrating they are root-n consistent, efficient, and multiply robust under certain conditions.",
    "main_topic": 2,
    "topic_score": 0.9940000176429749
  },
  {
    "filename": "2504.15870v1.pdf",
    "title": "Machine-learned RG-improved gauge actions and classically perfect gradient flows",
    "abstract": "Extracting continuum properties of quantum field theories from discretized spacetime is challenging due to lattice artifacts. Renormalization-group (RG)-improved lattice actions can preserve continuum properties, but are in general difficult to parameterize. Machine learning (ML) with gauge-equivariant convolutional neural networks provides a way to efficiently describe such actions. We test a machine-learned RG-improved lattice gauge action, the classically perfect fixed-point (FP) action, for four-dimensional SU(3) gauge theory through Monte Carlo simulations. We establish that the gradient flow of the FP action is free of tree-level discretization effects to all orders in the lattice spacing, making it classically perfect. This allows us to test the quality of improvement of the FP action, without introducing additional artifacts. We find that discretization effects in gradient-flow observables are highly suppressed and less than 1% up to lattice spacings of 0.14 fm, allowing continuum physics to be extracted from coarse lattices. The quality of improvement achieved motivates the use of the FP action in future gauge theory studies. The advantages of ML-based parameterizations also highlight the possibility of realizing quantum perfect actions in lattice gauge theory.",
    "main_topic": 3,
    "topic_score": 0.9919999837875366
  },
  {
    "filename": "2504.15925v1.pdf",
    "title": "Improving robustness and training efficiency of machine-learned potentials by incorporating short-range empirical potentials",
    "abstract": "Machine learning force fields (MLFFs) are powerful tools for materials modeling, but their performance is often limited by training dataset quality, particularly the lack of rare event configurations. This limitation undermines their accuracy and robustness in long-time and large-scale molecular dynamics simulations. In this work, we present a hybrid MLFF framework that integrates an empirical short-range repulsive potential and demonstrates improved robustness and training efficiency. Using solid electrolyte Li7La3Zr2O12 (LLZO) as a model system, we show that purely data-driven MLFFs fail to prevent unphysical atomistic clustering in extended simulations due to inadequate short-range repulsion. In contrast, the hybrid force field eliminates these artifacts, enabling stable long-time simulations, which are critical for studying various properties of LLZO. The hybrid framework also reduces the need for extensive active learning and performs well with just 25 training configurations. By combining physics-driven constraints with data-driven flexibility, this approach is compatible with most existing MLFF architectures and establishes a universal paradigm for developing robust, training-efficient force fields for complex material systems.",
    "main_topic": 9,
    "topic_score": 0.8550000190734863
  },
  {
    "filename": "2504.15964v1.pdf",
    "title": "Quantum machine learning advantages beyond hardness of evaluation",
    "abstract": "In recent years we have seen numerous rigorous proofs of quantum advantages in machine learning across various scenarios. The most general and least contrived advantages, involving data labeled by cryptographic or intrinsically quantum functions, rely on the infeasibility of classical polynomial-sized circuits to evaluate the true labeling functions. As a result, classical learning algorithms, with or without training data, cannot label new points accurately. While broad in scope, these results however reveal little about advantages stemming from the actual learning process itself. In cryptographic settings, additional insights are possible. By leveraging the property of randomgeneratability-the ability to generate data classically-proofs of classical hardness for identification tasks, where the goal is to \"just\" identify the labeling function behind a dataset, even if that function is not classically tractable, are possible. Identification tasks are especially relevant in quantum contexts, as Hamiltonian learning and identifying physically meaningful order parameters fit in this paradigm. However, for quantum functions, random-generatability has been conjectured not to hold, leaving no known identification advantages for genuinely quantum settings. In this work, we provide the first proofs of identification learning advantages for quantum functions under complexity-theoretic assumptions. First, we confirm the conjecture that quantum-hard functions are not random-generatable unless BQP is in the second level of the polynomial hierarchy, ruling out strategies analogous to those used for cryptographic functions. We then establish a new approach, first showing that verifiable identification-a task where the learner must rejects invalid datasets but solve the identification problem for valid ones -is intractable for classical learners, for quantum labeling functions, unless BQP is in the polynomial hierarchy. Then, we demonstrate that in specific task classes, the ability to solve just the identification problem implies verifiable identification within the polynomial hierarchy. This leads to our main result: there exists a broad class of identification tasks involving quantum functions that are efficiently solvable by quantum learners but intractable for classical learners unless BQP is contained in a low level of the polynomial hierarchy. These findings suggest that for many quantum-associated learning tasks, the entire learning process-not just final evaluation-gains significant advantages from quantum computation.",
    "main_topic": 6,
    "topic_score": 0.9959999918937683
  },
  {
    "filename": "2504.15993v1.pdf",
    "title": "Benchmarking machine learning models for predicting aerofoil performance",
    "abstract": "This paper investigates the capability of Neural Networks (NNs) as alternatives to the traditional methods to analyse the performance of aerofoils used in the wind and tidal energy industry. The current methods used to assess the characteristic lift and drag coefficients include Computational Fluid Dynamics (CFD), thin aerofoil and panel methods, all face trade-offs between computational speed and the accuracy of the results and as such NNs have been investigated as an alternative with the aim that it would perform both quickly and accurately. As such, this paper provides a benchmark for the windAI_bench dataset [1] published by the National Renewable Energy Laboratory (NREL) in the USA. In order to validate the methodology of the benchmarking, the AirfRANS dataset [2] is used as both a starting point and a point of comparison. This study evaluates four neural networks (MLP, PointNet, GraphSAGE, GUNet) trained on a range aerofoils at 25 angles of attack (4 • to 20 • ). to predict fluid flow and calculate lift coefficients (CL) via the panel method. GraphSAGE and GUNet performed well during the testing phase, but underperformed during validation. Accordingly, this paper has identified PointNet and MLP as the two strongest models tested, however whilst the results from MLP are more commonly correct for predicting the behaviour of the fluid, the results from PointNet provide the more accurate results for calculating CL.",
    "main_topic": 7,
    "topic_score": 0.9929999709129333
  },
  {
    "filename": "2504.16100v1.pdf",
    "title": "TOWARDS ACCURATE FORECASTING OF RENEWABLE ENERGY: BUILDING DATASETS AND BENCHMARKING MACHINE LEARNING MODELS FOR SOLAR AND WIND POWER IN FRANCE PREPRINT",
    "abstract": "Accurate prediction of non-dispatchable renewable energy sources is essential for grid stability and price prediction. Regional power supply forecasts are usually indirect through a bottom-up approach of plant-level forecasts, incorporate lagged power values, and do not use the potential of spatially resolved data. This study presents a comprehensive methodology for predicting solar and wind power production at country scale in France using machine learning models trained with spatially explicit weather data combined with spatial information about production sites' capacity. A dataset is built spanning from 2012 to 2023, using daily power production data from RTE (the national grid operator) as the target variable, with daily weather data from ERA5, production sites capacity and location, and electricity prices as input features. Three modeling approaches are explored to handle spatially resolved weather data: spatial averaging over the country, dimension reduction through principal component analysis, and a computer vision architecture to exploit complex spatial relationships. The study benchmarks state-of-the-art machine learning models as well as hyperparameter tuning approaches based on cross-validation methods on daily power production data. Results indicate that cross-validation tailored to time series is best suited to reach low error. We found that neural networks tend to outperform traditional tree-based models, which face challenges in extrapolation due to the increasing renewable capacity over time. Model performance ranges from 4 % to 10 % in nRMSE for midterm horizon, achieving similar error metrics to local models established at a single-plant level, highlighting the potential of these methods for regional power supply forecasting.",
    "main_topic": 3,
    "topic_score": 0.9950000047683716
  },
  {
    "filename": "2504.16131v1.pdf",
    "title": "Introduction to Quantum Machine Learning and Quantum Architecture Search",
    "abstract": "Recent advancements in quantum computing (QC) and machine learning (ML) have fueled significant research efforts aimed at integrating these two transformative technologies. Quantum machine learning (QML), an emerging interdisciplinary field, leverages quantum principles to enhance the performance of ML algorithms. Concurrently, the exploration of systematic and automated approaches for designing highperformance quantum circuit architectures for QML tasks has gained prominence, as these methods empower researchers outside the quantum computing domain to effectively utilize quantum-enhanced tools. This tutorial will provide an in-depth overview of recent breakthroughs in both areas, highlighting their potential to expand the application landscape of QML across diverse fields.",
    "main_topic": 3,
    "topic_score": 0.9869999885559082
  },
  {
    "filename": "2504.16135v1.pdf",
    "title": "Optimizing Post-Cancer Treatment Prognosis: A Study of Machine Learning and Ensemble Techniques",
    "abstract": "The aim is to create a method for accurately estimating the duration of postcancer treatment, particularly focused on chemotherapy, to optimize patient care and recovery. This initiative seeks to improve the effectiveness of cancer treatment, emphasizing the significance of each patient's journey and well-being. Our focus is to provide patients with valuable insight into their treatment timeline because we deeply believe that every life matters. We combined medical expertise with smart technology to create a model that accurately predicted each patient's treatment timeline. By using machine learning, we personalized predictions based on individual patient details which were collected from a regional government hospital named Sylhet M.A.G. Osmani Medical College & Hospital, Sylhet, Bangladesh, improving cancer care effectively. We tackled the challenge by employing around 13 machine learning algorithms and analyzing 15 distinct features, including Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, etc we obtained a refined precision in predicting cancer patient's treatment durations. Furthermore, we utilized ensemble techniques to reinforce the accuracy of our methods. Notably, our study revealed that our majority voting ensemble classifier displayed exceptional performance, achieving 77% accuracy, with LightGBM and Random Forest closely following at approximately 76% accuracy. Our research unveiled the inherent complexities of cancer datasets, as seen in the Decision Tree's 59% accuracy. This emphasizes the need for improved algorithms to better predict outcomes and enhance patient care. Our comparison with other methods confirmed our promising accuracy rates, showing the potential impact of our approach in improving cancer treatment strategies. This study marks a significant step forward in optimizing post-cancer treatment prognosis using machine learning and ensemble techniques.",
    "main_topic": 5,
    "topic_score": 0.9950000047683716
  },
  {
    "filename": "2504.16172v1.pdf",
    "title": "Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning",
    "abstract": "High-dimensional partial differential equations (PDEs) pose significant computational challenges across fields ranging from quantum chemistry to economics and finance. Although scientific machine learning (SciML) techniques offer approximate solutions, they often suffer from bias and neglect crucial physical insights. Inspired by inference-time scaling strategies in language models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML), a physics-informed framework that dynamically refines and debiases the SCiML predictions during inference by enforcing the physical laws. SCaSML leverages derived new physical laws that quantifies systematic errors and employs Monte Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to dynamically correct the prediction. Both numerical and theoretical analysis confirms enhanced convergence rates via compute-optimal inference methods. Our numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared to the base surrogate model, establishing it as the first algorithm to refine approximated solutions to high-dimensional PDE during inference. Code of SCaSML is available at  https://github.com/Francis-Fan-create/SCaSML .",
    "main_topic": 3,
    "topic_score": 0.6940000057220459
  },
  {
    "filename": "2504.16192v1.pdf",
    "title": "Probabilistic Emulation of the Community Radiative Transfer Model Using Machine Learning",
    "abstract": "The continuous improvement in weather forecast skill over the past several decades is largely due to the increasing quantity of available satellite observations and their assimilation into operational forecast systems. Assimilating these observations requires observation operators in the form of radiative transfer models. Significant efforts have been dedicated to enhancing the computational efficiency of these models. Computational cost remains a bottleneck, and a large fraction of available data goes unused for assimilation. To address this, we used machine learning to build an efficient neural network based probabilistic emulator of the Community Radiative Transfer Model (CRTM), applied to the GOES Advanced Baseline Imager. The trained NN emulator predicts brightness temperatures output by CRTM and the corresponding error with respect to CRTM. RMSE of the predicted brightness temperature is 0.3 K averaged across all channels. For clear sky conditions, the RMSE is less than 0.1 K for 9 out of 10 infrared channels. The error predictions are generally reliable across a wide range of conditions. Explainable AI methods demonstrate that the trained emulator reproduces the relevant physics, increasing confidence that the model will perform well when presented with new data.",
    "main_topic": 3,
    "topic_score": 0.9929999709129333
  },
  {
    "filename": "2504.16238v1.pdf",
    "title": "General Post-Processing Framework for Fairness Adjustment of Machine Learning Models",
    "abstract": "As machine learning increasingly influences critical domains such as credit underwriting, public policy, and talent acquisition, ensuring compliance with fairness constraints is both a legal and ethical imperative. This paper introduces a novel framework for fairness adjustments that applies to diverse machine learning tasks, including regression and classification, and accommodates a wide range of fairness metrics. Unlike traditional approaches categorized as pre-processing, in-processing, or post-processing, our method adapts in-processing techniques for use as a post-processing step. By decoupling fairness adjustments from the model training process, our framework preserves model performance on average while enabling greater flexibility in model development. Key advantages include eliminating the need for custom loss functions, enabling fairness tuning using different datasets, accommodating proprietary models as black-box systems, and providing interpretable insights into the fairness adjustments. We demonstrate the effectiveness of this approach by comparing it to Adversarial Debiasing, showing that our framework achieves a comparable fairness/accuracy tradeoff on real-world datasets.",
    "main_topic": 6,
    "topic_score": 0.8339999914169312
  },
  {
    "filename": "2504.16418v1.pdf",
    "title": "Scalable Data-Driven Basis Selection for Linear Machine Learning Interatomic Potentials",
    "abstract": "Machine learning interatomic potentials (MLIPs) provide an effective approach for accurately and efficiently modeling atomic interactions, expanding the capabilities of atomistic simulations to complex systems. However, a priori feature selection leads to high complexity, which can be detrimental to both computational cost and generalization, resulting in a need for hyperparameter tuning. We demonstrate the benefits of active set algorithms for automated data-driven feature selection. The proposed methods are implemented within the Atomic Cluster Expansion (ACE) framework. Computational tests conducted on a variety of benchmark datasets indicate that sparse ACE models consistently enhance computational efficiency, generalization accuracy and interpretability over dense ACE models. An added benefit of the proposed algorithms is that they produce entire paths of models with varying cost/accuracy ratio.",
    "main_topic": 4,
    "topic_score": 0.9890000224113464
  },
  {
    "filename": "2504.16493v1.pdf",
    "title": "Breaking scaling relations with inverse catalysts: a machine learning exploration of trends in CO 2 hydrogenation energy barriers",
    "abstract": "The conversion of CO 2 into useful products such as methanol is a key strategy for abating climate change and our dependence on fossil fuels. Developing new catalysts for this process is costly and time-consuming and can thus benefit from computational exploration of possible active sites. However, this is complicated by the complexity of the materials and reaction networks. Here, we present a workflow for exploring transition states of elementary reaction steps at inverse catalysts, which is based on the training of a neural network-based machine learning interatomic potential. We focus on the crucial formate intermediate and its formation over nanoclusters of indium oxide supported on Cu(111). The speedup compared to an approach purely based on density functional theory allows us to probe a wide variety of active sites found at nanoclusters of different sizes and stoichiometries. Analysis of the obtained set of transition state geometries reveals different structure-activity trends at the edge or interior of the nanoclusters. Furthermore, the identified geometries allow for the breaking of linear scaling relations, which could be a key underlying reason for the excellent catalytic performance of inverse catalysts observed in experiments.",
    "main_topic": 6,
    "topic_score": 0.9919999837875366
  },
  {
    "filename": "2504.16791v1.pdf",
    "title": "Radiometer Calibration using Machine Learning",
    "abstract": "Radiometers are crucial instruments in radio astronomy, forming the primary component of nearly all radio telescopes. They measure the intensity of electromagnetic radiation, converting this radiation into electrical signals. A radiometer's primary components are an antenna and a Low Noise Amplifier (LNA), which is the core of the \"receiver\" chain. Instrumental effects introduced by the receiver are typically corrected or removed during calibration. However, impedance mismatches between the antenna and receiver can introduce unwanted signal reflections and distortions. Traditional calibration methods, such as Dicke switching, alternate the receiver input between the antenna and a wellcharacterised reference source to mitigate errors by comparison. Recent advances in Machine Learning (ML) offer promising alternatives. Neural networks, which are trained using known signal sources, provide a powerful means to model and calibrate complex systems where traditional analytical approaches struggle. These methods are especially relevant for detecting the faint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is one of the main challenges in observational Cosmology today. Here, for the first time, we introduce and test a machine learning-based calibration framework capable of achieving the precision required for radiometric experiments aiming to detect the 21-cm line.",
    "main_topic": 2,
    "topic_score": 0.9929999709129333
  }
]